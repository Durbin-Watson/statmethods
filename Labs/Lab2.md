# Лабораторная работа 2 "Нейросетевые методы аппроксимации"

Срок сдачи: 28 марта, оценка: 20 баллов, штраф: 2 балла в неделю.

Целью работы является решение задачи аппроксимации произвольной зависимости искусственными нейронными сетями, а также исследование точности аппроксимации в зависимости от конфигурации нейронной сети и представления обучающей выборки.

1. Подготовка исходных данных. Возьмите выборку из предыдущей лабораторной работы, поделенную на две части: обучающую и тестовую. Желательно, чтобы данные представляли собой временной ряд. Если данные возьмете не из предыдущей лабораторной работы, то придется заново построить регрессионную и интерполяционную модели и расчитать коэффициенты корреляции (детерминации). Проведите нормировку исходных данных, то есть линейное преобразование всех значений факторов и откликов таким образом, чтобы их значения попадали в сопоставимые по величине интервалы

2. Структурная адапатация. Зафиксируйте начальную структуру (1, m, 1) нейросетевой модели, где m - число нейронов в скрытом слое. Проведите параметричестую идентификацию рассматриваемой модели по обучающей выборке, расчитайте коэффициент корреляции (детерминации) по обучающей выборке. Повторяйте предыдущий эту процедуру, варьируя количество нейронов m внутреннего слоя в диапазоне от 1 до 100. Постройте график зависимости выбранного функционала качества (суммы квадратов ошибок, детерминации) от числа нейронов в скрытом слое. Получите оптимальную (минимизирующую значение функционала качества) структуру нейронной сети по этому графику.

3. Анализ результатов. Постройте графики по нейросетевой, интерполяционной и регрессионных моделям на основе обучающей и тестовой выборкам. Сделайте вывод о точности построенных моделей по графикам и коэффициентам корреляции.


## Вопросы к защите

1. Формальный нейрон Маккалока-Питтса.
2. Персептрон Розенблатта. Теорема об обучении персептрона.
3. Параметрическая и структурная идентификация.
4. Подходы к параметрической идентицикации: обучения с учителем (supervised), без учителя (non-supervised), гибридный (semi-supervised) с подкреплением (reinforcement learning).
5. Обучение с учителем: аппроксимация многомерных функций. Теорема Колмогорова-Арнольда. 
6. Алгоритм обратного распространения ошибки. Связь с методом наименьших квадратов.
7. Проблемы обучения: ошибки аппроксимации, переобучение.
8. Структурная идентифиация. Виды архитектур.
9. Адаптивная оптимизации архитектуры сети.
10. Предварительная обработка входных векторов: нормирование и кодирование.


## Рекомендуемые источники

1. Нейронная сеть прямого распространения в Python
http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor

